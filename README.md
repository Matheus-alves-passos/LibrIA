# ğŸ–ï¸ Classificador de Sinais com MÃ£os (MediaPipe + Random Forest)

Este projeto tem como objetivo capturar sinais manuais por meio da webcam, treinar um modelo de **machine learning** para reconhecÃª-los e, por fim, usar esse modelo em tempo real com feedback por **sÃ­ntese de voz**.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```
.
â”œâ”€â”€ collect_img.py         # Captura imagens da webcam e salva em pastas
â”œâ”€â”€ create_dataset.py      # Processa imagens salvas e cria um dataset .pickle
â”œâ”€â”€ train_classifier.py    # Treina modelo RandomForest com os dados coletados
â”œâ”€â”€ interface_classifier.py# Interface final: previsÃ£o em tempo real + fala
â”œâ”€â”€ data/                  # DiretÃ³rio gerado contendo subpastas por classe (0, 1, 2, ...)
â”œâ”€â”€ data.pickle            # Arquivo de dataset gerado com dados + rÃ³tulos
â”œâ”€â”€ model.p                # Modelo treinado salvo
```

---

## ğŸ“¦ DependÃªncias

Instale os pacotes necessÃ¡rios com:

```bash
pip install opencv-python mediapipe scikit-learn numpy pyttsx3 matplotlib
```

---

## ğŸ“¸ Etapa 1: Coleta de Dados (`collect_img.py`)

- Usa a **webcam** para capturar imagens.
- Para cada classe (gesto), cria uma pasta separada.
- Aguarda o usuÃ¡rio apertar `Q` para iniciar a coleta.
- Captura 100 imagens por classe.

> ğŸ“ Resultado: pasta `./data/` contendo subpastas `0`, `1`, `2`, ... com imagens `.jpg`.

---

## ğŸ§  Etapa 2: CriaÃ§Ã£o do Dataset (`create_dataset.py`)

- LÃª todas as imagens salvas.
- Usa o **MediaPipe** para detectar mÃ£os nas imagens.
- Extrai coordenadas normalizadas dos pontos da mÃ£o.
- Associa cada conjunto de coordenadas ao rÃ³tulo da classe correspondente.

> ğŸ’¾ Resultado: `data.pickle` com as listas `data` (features) e `labels` (classes).

---

## ğŸ‹ï¸ Etapa 3: Treinamento do Modelo (`train_classifier.py`)

- Carrega o `data.pickle`.
- Divide em treino (80%) e teste (20%).
- Usa **RandomForestClassifier** para treinar.
- Exibe a precisÃ£o final do modelo.
- Salva o modelo em `model.p`.

> ğŸ¯ Exemplo de saÃ­da:
```
98.75% dos dados foram classificados com sucesso !
```

---

## ğŸ§ ğŸ—£ï¸ Etapa 4: Interface com PrevisÃ£o e Fala (`interface_classifier.py`)

Este Ã© o sistema final interativo que:

1. Inicia a webcam em **tela cheia**.
2. Usa o **MediaPipe** para detectar gestos da mÃ£o ao vivo.
3. Normaliza os pontos da mÃ£o e faz a prediÃ§Ã£o com o modelo `model.p`.
4. Mostra o gesto reconhecido na tela e desenha a mÃ£o.
5. Usa **pyttsx3** para **falar** o gesto detectado com voz em portuguÃªs.

> âœ… Pressione **Q** para encerrar a execuÃ§Ã£o.

### ğŸ—£ï¸ Exemplo de Frases Reconhecidas
- â€œOiâ€
- â€œMeuâ€
- â€œNomeâ€
- â€œMatheusâ€

Estes rÃ³tulos sÃ£o definidos no dicionÃ¡rio:

```python
labels_dict = {0: 'Oi', 1: 'Meu', 2: 'Nome', 3: 'Matheus'}
```

---

## ğŸ§ª SugestÃ£o de Fluxo de Uso

1. Rode `collect_img.py` para capturar imagens por classe.
2. Rode `create_dataset.py` para gerar o arquivo de dataset.
3. Rode `train_classifier.py` para treinar e salvar o modelo.
4. Rode `interface_classifier.py` para usar o sistema ao vivo com previsÃ£o e voz.

---

## ğŸ’¡ Ideias de ExpansÃ£o

- Adicionar mais classes (gestos).
- Usar CNN em vez de Random Forest para maior precisÃ£o.
- Adicionar modo de "frase contÃ­nua" com buffer de palavras.
- Exportar frases detectadas para um arquivo `.txt`.

---

## ğŸ“š CrÃ©ditos

Este projeto utiliza as seguintes tecnologias:

- [MediaPipe](https://mediapipe.dev/)
- [OpenCV](https://opencv.org/)
- [scikit-learn](https://scikit-learn.org/)
- [pyttsx3](https://pyttsx3.readthedocs.io/)
